# S4L: Intelligent Question Answering in Restricted Domains Using Deep Learning and Question Pair Matching
With the rapid expansion of the Internet, intelligent question answering for information retrieval has once again gained widespread attention. However, current question answering models mainly focus on the general and common-sense questions in open domains and are incapable to effectively solve more complex professional domain questions. This paper proposed an integrated framework for Chinese
intelligent question answering in restricted domains. The proposed model fused convolutional neural network and bidirectional long short-term memory network which performs efficient semantic analysis on the question pairs to extract more effective features of the text. Meanwhile, the coattention mechanism and attention mechanism were combined to obtain the semantic interaction and feature representation of the question pair for providing complete information for subsequent calculations. In addition, we introduced the
method of question pair matching to implement the Chinese intelligent question answering in a restricted domain. Experiments were tested and evaluated on the open-source CCKS2018 dataset and our private self-built inverted pendulum control question answering (IPC-QA) dataset for automation control virtual learning environment. Experimental results confirm that the proposed models are efficient and achieve a high precision of 0.86042 and 0.8031 on CCKS2018 and IPC-QA respectively.

## 1.INTRODUCTION
BiLSTM network can better construct the vector representation of the answers based on the input questions, but the model ignores the influence of the answers on the question vector representations and results in certain result deviations.On the other hand, at present the open domain intelligent question answering has made remarkable progress in the field of deep learning research. However, questions in a restricted domain are difficult to process because the related knowledge base contains specialized and complex specific corpus data of
the corresponding domain, which is a bottleneck hindering the development of intelligent question answering. As the knowledge system has strong logic and truth in a particularly professional field, the standard answers corresponding to the questions are mostly limited and unique, and the questions corresponding to a specific answer have multiple representations. The method of modeling and matching between question and question is used to solve the dilemma of one answer corresponding to multiple question representations in a restricted domain. The main advantages are as follows: 

(1) The stability of corpus. In the restricted domain where the underlying knowledge base or dataset domain is vertical and closed, the question corresponds to the answer one to one, and the data size is delineated in an inductive range, while the user-input questions have different forms and expressions with a tremendous number [14]. By modeling the question pair, the input question is matched with the existing question, then the corresponding answer output is obtained.
(2) Semantic space aspect. There may be a semantic gap when coding between the questions and answers. Whereas when multiple questions correspond to a same answer, their starting point and form in the learning process are not far from each other, thus their semantic space is consistent. 
(3) On-line running speed. The questions corresponding to similar dense vectors in the model can be indexed with tools such as artificial intelligence markup language (AIML) through the idea of question pair matching, saving computing resources and increasing the running speed.the main contributions of this paper are summarized as follows:

(1) An integrated framework for Chinese intelligent question answering in restricted domains was presented based on CNN-BiLSTM network, coattention mechanism and attention mechanism. We fused CNN and BiLSTM to extractand represent important textual information and contextual information. The coattention mechanism and attention mechanism were combined to obtain the semantic interaction and feature representation of the question pair for providing complete information for subsequent calculations. Additionally, we introduced the method of question pair matching to implement the Chinese intelligent question answering in a restricted domain. The proposed methods can not only effectively achieve a high accuracy, but also improve resource utilization.

## 2.RELATED WORK
CNN only combines consecutive phrases and only considers the word order in the sliding window, resulting that it is impossible to capture long-distance dependencies and can’t represent complex semantic sentences well. Inspired by the above works, we combined CNN and BiLSTM to effectively capture data and information of local keywords in the sentence. At the same time, the model can also achieve the preliminary analysis and understanding of the relevance of each word in the Chinese sentence.هn this paper we combined coattention mechanism and attention mechanism to learn the features of the input question and the existing question together to obtain the interactive vector representation of the question pair.

### 2.1. ROPOSED MODEL FOR QUESTION ANSWERING
The Chinese intelligent question answering based on question pair matching is a progressive process. Our proposed framework for Chinese intelligent question answering in restricted domains is shown in Fig. 1. After the input corpus is preprocessed by the Chinese word segmentation tool, we encode and extract features of different expressions of the same target. Firstly, it needs a well-organized underlying question answering knowledge base to train the sematic representation and matching model for the question pairs. Secondly,
we analyze the semantics and structure of the question pairs to construct the feature vector model. This is a crucial step.Finally, the matching algorithm is deployed to calculate the similarity between the input question vector and the existing question vector, then outputting the answer.The model of Chinese sentence semantic feature extraction is constructed in Fig. 2. Firstly, the sentences after word segmentation are sent to the word2vec model trained by the self-built corpus dataset for encoding. With a layer of convolutional neural network, the model can fully consider the word order and semantic context of the input sentence.Then the local key information of the statement is extracted and compressed into a fixed length to prepare for semantic interaction and representation of question pair in subsequent network.

Next, semantic information is extracted and feature vectors are further represented by combining stacked BiLSTM network and coattention mechanism. This method not only can solve the dependence problem between the before and after words in the long statements, but also can
obtain the related feature representation between the question pair. Finally, the output is predicted by the softmax function.
Chinese sentences are more complex and diverse in expression, and there are also certain difficulties in the specific lexical parsing and coding of restricted domains. Compared with the traditional feature extraction models, the network architecture in Fig. 2 can solve the problem of Chinese semantic understanding. CNN is used to capture crucial feature information, while the BiLSTM network is used to obtain
the semantic analysis of the entire sentence.

![Fig. 1](https://github.com/mohammadreza76/read-papers/blob/main/Intelligent%20Question%20Answering%20in%20Restricted%20Domains%20Using%20Deep%20Learning%20and%20Question%20Pair%20Matching/1.png)

![Fig. 2](https://github.com/mohammadreza76/read-papers/blob/main/Intelligent%20Question%20Answering%20in%20Restricted%20Domains%20Using%20Deep%20Learning%20and%20Question%20Pair%20Matching/2.png)

### 2.1.1. SEMANTIC ANALYSIS WITH THE FUSION STRUCTURE OF CNN AND BILSTM
The input of the CNN network is the n ×k-order word matrix generated by the pre-trained word2vec model, where n is the number of all words in the sentence, i.e., the maximum length of the sentence, and k is the vector dimension corresponding to each word. In this paper, the dimension of the word2vec model is trained as k =300. We set xi ∈Rk as the k-dimensional word vector of the i-th word of the sentence,
where the vector of the unregistered word is filled by the zero padding method, then the sentence dense vector of length n is expressed as:
x1:n =x1 ⊕x2 ⊕···⊕xn

where ⊕ represents the word vector parallel concatenation operator and xa:b represents the word vector matrix composed of {xa,x,...,xb}.
in the process of natural text processed by convolutional neural network, the convolution kernel generally covers the words of the upper and lower several lines of a statement,so the convolution kernel of a text has only length but no width. We set the convolution kernel window W ∈R^h×k with a fixed length h (h×k, the longitudinal h, and the transverse k represent the convolution window size, the number of words contained in the sliding window, and the dimension of the word vector, respectively) to convolve the words contained in xi:i+h−1 in the sentence. The feature operation of the convolution kernel window on the i-th word of the sentence is calculated as follows:
ci =σ(W ·xi:i+h−1 +b)

where ci, σ, xi:i+h−1,and b represent the feature output of the word i after convolution, the activation function, the matrix composed of the word vectors from i to i +h −1, and the bias factor, respectively;W is the operational matrix of the convolutional layer, i.e., the sliding window.

By analogy, when the convolution kernel window with longitudinal length h is acted on the entire sentence with length n, n −h +1 new feature vectors can be obtained for the statement, which form the one-dimensional feature map of the corresponding statement. The specific calculation is as follows:
c =[c1,c2,...,cn−h+1] 

In this paper, we extract the maximum eigenvalues of the convolution kernel vectors after convolution by the 1-max pooling strategy, and combine them as input of the BiLSTM network.Consequently, the final combined output vector C obtained after pooling is a fixed-size vector matrix whose dimension is the number of feature maps output from convolutional layer.Through convolution and pooling, the vector matrix of
the statement performs initial semantic analysis and feature extraction to obtain local important information features of the statement and effectively reduce the training parameters.After extracting the structured semantic information with representational ability in the sentences of question pair Q = (q1,q2,...,qn) and Q′ = (q′1,q′2,...,q′m) through CNN network, the output is expressed as CQ =max[c1,c2,...,cn−h+1] and CQ′ =max[c′1,c′2,...,c′m−h+1],where n and m are the numbers of words contained in question Q1 and question Q2, respectively, namely the sentence length,and h is the convolution window size. The next step is to transfer CQ and CQ′ to BiLSTM network to extract the
interdependencies and influences of the distant words in the sentence.

this paper construct a stacked BiLSTM network based on the LSTM cell to fully achieve semantic parsing of a single statement. We define the
output of upper level BiLSTM network, yt, as the input of next level BiLSTM network. The stacked BiLSTM network structure is shown in Fig. 4.
![Fig. 4](https://github.com/mohammadreza76/read-papers/blob/main/Intelligent%20Question%20Answering%20in%20Restricted%20Domains%20Using%20Deep%20Learning%20and%20Question%20Pair%20Matching/4.png)


We define pt and p′t to represent the t-th vectors of the question pair sequences CQ and CQ′, respectively.The question pair sequences after CNN network are sent to the stacked BiLSTM network separately, from which their state matrices HQ and HQ′.


### 2.1.2. SEMANTIC INTERACTION AND FEATURE REPRESENTATION USING COATTENTION AND ATTENTION MECHANISMS
In this paper, we design an incidence matrix in the coattention mechanism to capture the correlation and interaction between vectors, and use the softmax activation function to map the output of multiple neurons to the corresponding interval range. The state matrices HQ and HQ′ of question pair through the stacked BiLSTM network are taken as input, and the internal structure of the coattention mechanism is shown
in Fig. 5.
![Fig. 5](https://github.com/mohammadreza76/read-papers/blob/main/Intelligent%20Question%20Answering%20in%20Restricted%20Domains%20Using%20Deep%20Learning%20and%20Question%20Pair%20Matching/5.png)

We perform matrix multiplication on the state matrices HQ and HQ′ to calculate the correlation matrix L. Each item in incidence matrix is the correlation score between the words of the question pair sentences, that is, the interaction between the question pair statements is reflected as follows: (13)formula in page 7

then use softmax.In the process of data calculation and transmission of the stacked BiLSTM network and the coattention mechanism, there may be some information mismatches in the question pair statements. To solve this problem, the attention mechanism layer is added following the coattention mechanism to connect and integrate the statement vector information of the previous few steps.The input of the last module is the new eigenvector representations of the question pair statements CQ and CQ′, and CQt is set as the t-th attention feature vector of the input question statement. We use max pooling to convert the input into a fixed-length vector Oq, which is the final feature vector
output of the input question statement. When the attention mechanism is used to analytically represent the final vector of the existing question statement, the softmax weights of all the feature vectors (CQ′1 ,CQ′2 ,...,CQ′m ) in the existing question statement can be calculated and obtained according to Oq.

After allocated weights, the final feature vector output Oq′ of the existing question statement can be obtained based on the attention mechanism model. Then, attention mechanism can integrate statement information, with which we can acquire the final outputs of the semantic parsing of the question pair as follow: fomula 17 & 18 
where Sq′q is standardized by softmax function to represent the attention weight of the text vector of the existing question statement at time t, which is proportional to CQ′t . The higher the value of Sq′q, the higher the relevance between CQ′ t and the input question statement, that is, the existing question statement will have a greater effect on the eigenvector representation of the input question.

### 2.1.3. QUESTION PAIR MATCHING
After obtaining the final eigenvector representations of the question pair, the output is determined by measuring the similarity between the feature vectors, of which cosine similarity and Euclidean distance are commonly used. The measure standard of cosine similarity is the spatial angle between two vectors, while Euclidean distance is to calculate the absolute spatial distance of two vectors. The spatial dia-
gram of cosine similarity and Euclidean distance is shown in Fig. 6.
Generally, it is hoped that the angle between the feature vectors of the question pair is small enough and the distance is the shortest so as to maximize the similarity capture and calculation of the question pair statement vectors. Therefore, this paper reconciles cosine similarity and Euclidean distance to generate a similarity calculation function that can consider the above both factors simultaneously. The
calculation functions of the final vector similarity are as follows: (in Fig. 6.)

![Fig. 6](https://github.com/mohammadreza76/read-papers/blob/main/Intelligent%20Question%20Answering%20in%20Restricted%20Domains%20Using%20Deep%20Learning%20and%20Question%20Pair%20Matching/6.png)

### 2.1.3. EXPERIMENTAL SETTINGS AND PARAMETER OPTIMIZATION

Some data of training sets in CCKS2018 and IPC-QA are sorted out to complete the training of the word2vec model.We set the dimension of the output word vector to 300 and the output length of each question statement to 35. The OOV (Out-Of-Vocabulary) zero-padding method is used to make up the insufficient length.There are two different convolution windows with h of 2 and 3 respectively in the CNN network, which slide on the statement matrix by the stride of 1. If the number of convolution kernels for both windows is 100, 200 feature maps can be generated. Therefore, each question sentence can obtain corresponding output of a 200-dimensional matrix after max pooling, which is the feature vector input of the stacked BiLSTM network.

We set the initial learning rate of the model, the IPC-QA data batch size, the loss function’s fixed margin M, and the regularization parameter λ to 0.001, 30, 0.2, and 1e-5,respectively. During the training process, a dropout layer with a dropout rate of 0.5 is added to the convolutional layer to avoid over-fitting of the model, a gradient clipping method with a clip gradient of 5 is used to avoid the model gradient explosion, and the hinge loss function is set as the objective function of the model training. In this section, Adam algorithm is used to optimize the model with the decay rate of 0.95 to complete the updating and optimization of network parameters. 

![Fig. 7](https://github.com/mohammadreza76/read-papers/blob/main/Intelligent%20Question%20Answering%20in%20Restricted%20Domains%20Using%20Deep%20Learning%20and%20Question%20Pair%20Matching/7.png)

These data strongly proved that the coattention mechanism can effectively bridge the lexical gap between question pair sentences. A reasonable structure of the attention mechanism can help the NLP task to extract the key information of the sentence.However, the final experimental precision is not good enough, of which one reason may be that the IPC-QA dataset corpus is not ample, resulting that the semantic understanding of the negative cases of the question pairs is not sufficiently differentiated by the model.

The idea of question pair matching and the construction of inverted pendulum dataset have laid a solid foundation for further research on intelligent question answering. However, the model of this paper still has some defects in the expansion of professional field and dataset. In addition, although the model has achieved good results in experiments, it lacks practical applications in reality.
