# S4L: DeepRank Paper
this paper concerns a deep learning approach to relevance ranking in information retrieval (IR). Existing deep IR models such as DSSM
and CDSSM directly apply neural networks to generate ranking scores, without explicit understandings of the relevance.human judgement process, a relevance label is generated by the following three steps: 1) relevant locations are detected; 2) local relevances are determined; 3) local relevances are aggregated to output the relevance label..DeepRank, to simulate the above human judgment process. Firstly, a detection strategy is designed to extract the relevant contexts. Then, a measure network is applied to determine the local relevances by utilizing a convolutional neural network (CNN) or two-dimensional gated recurrent units (2D-GRU). 

## 1.itroduction
Relevance ranking is a core problem of information retrieval.successful learning to rank algorithm usually relies on effective handcrafted features for the learning process. the feature engineering work is usually time-consuming,incomplete and over-specified, which largely hinder the further development of this approach.DSSM, CDSSM, and DRMM. Both DSSM and CDSSM directly apply deep neu-
ral networks to obtain the semantic representations of query and document, and the ranking score is produced by computing their
cosine similarity.DSSM and CDSSM only consider the semantic matching between query and document, but ignore the more important relevance matching characteristics,such as exact matching signals, query term importance, and diverse matching requirement.DRMM does
not explicitly model the relevance generation process, and fails to capture important IR characteristics such as passage retrieval
intrinsics and proximity heuristics.Consequently, DeepRank contains three parts to simulate the human judgement process.Where does the relevance occur? According to the query-centric assumption proposed in, the relevant information for a query only locates in the contexts around query terms. erefore, the context with a query term at the center position, namely query-centric context, is recognized as the relevant location in the detection step.How to measure the local relevance? Aer the detection step,a measure network is utilized to determine the local relevance between query and each query-centric context. Firstly, a tensor is constructed to incorporate both the word representations of query/query-centric context, and the interactions between them. then a CNN or 2D-GRU is applied on the tensor to output the representation of the local relevance. In this way, important IR characteristics such as exact/semantic matching signals, passage retrieval intrinsics, and proximity heuristics can be well captured. How to aggregate such local relevances to determine the global relevance score? As shown, two factors are important for user’s complex principles of aggregating local relevances, i.e. query term
importance and diverse relevance requirement. therefore we propose to first aggregate local relevances at query term level, and then make the combination by considering weights of different terms, via a term gating network. To obtain the term level relevance, we first group the query-centric contexts with the same central word together. then a recurrent neural network (RNN) such as GRU  or LSTM is utilized to aggregate such local relevances sequentially, by further considering the position information of these query-centric contexts in the whole document.

Another related sort of deep models, ourishing in NLP, provide a new way of thinking if we treat IR task as a general text matching
task, i.e. query matches document. these work can be mainly categorized as representation focused models and interaction focused
models. the representation focused models try to build a good representation for each single text with a neural network, and then
conduct matching between the two vectors. the DSSM and CDSSM mentioned above belong to this category. ere are also some other
ones such as ARC-I model, which builds on word embeddings and makes use of convolutional layers and pooling layers to ex-
tract compositional text representation. e interaction focused models first build the local interactions between two texts, and
then use neural networks to learn the more complicated interaction patterns for matching. Typical examples include ARC-II and
MatchPyramid, and Match-SRNN . these models have been shown effective in text matching tasks such as paraphrase identication and question answering. DRMM can also be viewed as an interaction focused model.

## 2.DeepRank algorithm
In the detection step, the query-centric contexts are extracted to represent where the relevance occur. In the measurement step, CNN or 2D-GRU is adopted to measure the local relevance between query and each query-centric context.Finally in the aggregation step, RNN and a term gating network are utilized to aggregate those local relevances to a global one for ranking. Figure 1 gives an illustration of DeepRank.We will first give some mathematical notations. Each query and document are represented as a sequence of word q =(w1,...,wM )
and d =(v1,...,vN ), where wi denotes the i-th word in the query and vj denotes the j-th word in the document.we can use d(k )[p]=(vp−k ,··· ,vp ,··· ,vp+k ), Mathematically, given a query q and document d, if query term wu appears at the p-th position in the document, i.e. wu =vp , the query-centric context centered on this query term is represented as spu (k)=dk [p].

we propose a new measure network, as shown in Figure 2. Firstly a tensor is constructed as the input. then CNN or 2D-GRU is applied on the tensor to output a vector, which stands for the representation of local relevance. In this way, the important IR characteristics such as exact/semantic matching signals, passage retrieval intrinsics, and proximity heuristics can be well captured in the measurement step.

![ Figure 1]()

### 2.1. Input Tensor
the key idea of this layer is to feed both the word representations of query/query-centric context and the interactions between them into the input of the measure network.Specifically for a given query q and query-centric context spu (k) with wu =vp , we denote the word-level interaction matrix used in MatchPyramid and Match-SRNN as S, where each element Si j is defined as the similarity of corresponding words wi and vj . For example, indicator function or cosine similarity can be used to capture the word-level exact or semantic matching signals.

Sindi j =1 if wi =vj ,Sindi j =0 otherwise,
Scosi j =xiT yj/(‖xi‖·‖yj‖),

where xi and yj denote the word embeddings of wi and vj , respectively. To further incorporate the word representations of query/query-centric context to the input, we extend each element of Si j to a three-dimensional vector  ̃Si j =[xi ,yj ,Si j ]T .therefore,
the original matrix S will become a three-order tensor, denoted as S.which include three matrices, i.e. query matrix, query-centric context matrix, and word-level interaction matrix.Based on the input tensor S, various neural networks can be directly applied to obtain the representations for the local relevance between query and query-centric context.In this paper, we choose the CNN architecture in MatchPyramid and 2D-GRU architecture in Match-SRNN, mainly because they have the ability to capture important proximity heuristics for IR.

### 2.2. Convolutional Neural Network
convolution operation can extract various matching paerns from the input tensor S, by using different kinds of kernels, as shown in Figure 2. then a max-pooling operation is used to lter significant matching paerns for further relevance determination in measurement step.
(3) see formula in covolutional network

where l denotes the l -th slide of the tensor, γ denotes the fixed size of K different kernels, (S(l ) i +s,j+t) denote the (i +s,j +t )element of the l -th matrix of the input tensor S, wκs,t and bκdenotes parameters.Finally, all the signicant matching patterns obtained from different kernels are concatenated to form a vector, i.e. h =[h(1),··· ,h(K )]T , to represent the local relevance.

![ Figure 2]()

### 2.3. Two-Dimensional Gated Recurrent Units

Rather than using a hierarchical structure to capture the matching paerns, 2D-GRU in Match-SRNN adopts a different sequential model to
accumulate the matching signals. It is an extension of GRU (a typical variant of RNN) to two-dimensional data like matrix or tensor1. Specifically, 2D-GRU scans from top-le to boom-right (or in a bidirectional way) recursively. At each position, the hidden representation depends on the representations of the top, left, diagonal and current positions in the matrix.
Please note that both CNN and 2D-GRU well capture the proximity heuristics in IR. Proximity heuristic rewards a document where the matched query terms occur close to each other, which is an important factor for a good retrieval model. For CNN, if the matched
query terms occur close to each other, appropriate kernels can be utilized to extract such signicant matching paerns and inuence
the relevance score. In this way, CNN well captures the proximity heuristics. 2D-GRU can also model proximity. When there is a doc-
ument where the matched query terms occur close to each other,the representation h will be strengthened by appropriately setting gates and other parameters. As a result, the relevance score of the document will be increased.

## 3.Aggregation Network

we need a further aggregation step to output a global relevance score. In this process, two IR principles are going to be considered in our deep architecture. One is query term importance: query terms are critical to express user’s information need and some terms are more important than others . the other one is diverse matching requirement: the distribution of matching patterns can be quite different in a relevant document.In order to capture the two IR principles, we first conduct a query term level aggregation, in which the diverse
matching requirement is taken into account. then a term gating network is applied to capture the importance of different terms when producing the global relevance score.

### 3.1. query Term Level Aggregation.

In order to capture the principle of diverse relevance requirement, we need to consider the position of the corresponding query-centric context when conducting query term level aggregation. therefore, we append each vector h with the position indicator to encode the position information of the corresponding query-centric context. Specifically, different position functions g(p)are utilized in our aggregation network:

Constant Function: g(p)=C,C ∈R,
Linear Function: g(p)=(L −p)/L,L ∈R,
Reciprocal Function: g(p)=a/(p +b),a,b ∈R,(6)
Exponential Function: g(p)=a ·exp(−p/b),a,b ∈R,

where p stands for the position of the query-centric context, determined by the central word vp . Aer this appending operation,the representation of local relevance for a query-centric context centered at word vp (denoted as h(p)) becomes [h(p)T ,g(p)]T .
To conduct query term level aggregation, we first group h(p) with the same central word together, which stands for all the local
relevances with respect to a same query term. then RNN is used to integrate such local relevances by considering position information
into consideration. that is to say, we can obtain the global relevance representation T(wu )for each query term wu as follows.see (7) formula

### 3.2. Term Gating Network for Global Aggregation.

Based on query term level global relevance representations T(wu), we use a term gating network (similar to that used in DRMM) to obtain
the final global relevance score, by considering importances of different query terms. Specically, we dene a weight parameter
Ewu for each query term, and linear combine all the query term level relevances as follows.see (8) formula


## 4.Model Training

DeepRank is an end-to-end deep neural network, which can be trained using stochastic gradient decent (SGD) methods, such as Adam. L2 regularization and early stopping strategy are also used in our implementation to deal with overfiting. More implementation details will be given in the experiments.In our experiments, we use the following pairwise hinge loss for training, since we are considering a ranking problem. For future work, we are also willing to try other pairwise losses and listwise loss functions to conduct the training process.

## 5.EXPERIMENTS

In this section, we conduct extensive experiments to evaluate DeepRank against state-of-the-art models, including learning to rank methods, and existing deep learning methods. the experimental results on both LETOR4.0 benchmark and a large scale clickthrough data show that our model can significantly outperform all the baselines, especially when other existing deep learning methods perform much worse than learning to rank methods. Furthermore, we give detailed experimental analysis to show more insights on our model.

### 5.1.Baseline Methods

We adopt two types of baseline methods for comparison, including learning to rank methods and deep learning methods.For learning to rank approach, we compare both pairwise and list-wise ranking methods. the pairwise baselines include RankSVM and RankBoost, which apply SVM and boosting techniques to the pairwise ranking problem, respectively. the listwise baselines include AdaRank and LambdaMart, where AdaRank proposes to directly optimizing IR evaluation measures by boosting to obtain a ranking list, and LamdaMart uses gradient boosting
for optimizing a listwise ranking loss.

For deep learning approach, we compare three existing deep IR models, i.e. DSSM , CDSSM, and DRMM. We also compare some popular deep methods for text matching, including one representation focused method, i.e. ARC-I, and three interaction focused methods, i.e. ARC-II, MatchPyramid,and Match-SRNN. 

SQA model combines handcraft features in the learning process, therefore it is not appropriate to directly compare it with DeepRank, which only uses automatically learned feature from raw text for ranking. For fair comparison, we delete the handcrafted features in SQA and obtain a pure deep SQA model, denoted as SQA-noFeat. Furthermore, we incorporate the handcrafted features (46 default features in LETOR4.0) into the last layer of DeepRank to obtain DeepRank-Feat, which is used to compare with SQA.

the DeepRank4 for performance comparison is implemented using the following seings: the window size of query-centric context is set to 15; cosine similarity is adopted to construct the input tensor; both CNN and 2D-GRU are used in the measurement step, therefore we have two versions of DeepRank, denoted as DeepRank-CNN and DeepRank-2DGRU; the reciprocal function is used as thepositional function, and GRU is adopted in the aggregation step. We also compare different seings of DeepRank for detailed analysis.

### 5.2. Evaluation Measures

For the evaluation on LETOR4.0, we follow the data partitions on this dataset (5-fold) and the average results are reported.three evaluation measures are used in this paper,i.e. Precision, NDCG, and MAP. Furthermore, we conduct a pairwise t-test for significance testing with p-value lower than 0.05 (i.e. p-value≤0.05).

### 5.3. Performance Comparison

None of existing deep learning models could perform comparably with learning to rank methods. Some of them are even worse than BM25.the results tell us that the automatically learned features in existing deep learning models are not better than traditional extracted ones, though they are using more complex models for training. Someone may argue that the experimental findings are inconsistent with previous studies that DSSM and CDSSM can significantly outperform traditional retrieval models, as stated in and. the reason lies in that
LETOR4.0 is much smaller than the clickthrough data.In the following experiments on ChineseClick, we can see that all the deep models perform better than BM25. therefore,deep models usually need more data for optimization, which is also the reason why we do not use Robust04 and ClueWeb-09-CAt-B used in (another model) for evaluation.

As for the comparisons between these deep models, interaction focused perform much better than representation focused. this is consistent with the understanding that interaction signals are much more important than the semantic representation of query/document in IR.
Furthermore, DRMM performs the best among all the deep learning baseline methods.this is because DRMM further incorporate IR characteristics into their architecture, which indicate the importance of capturing IR intrinsics in the architecture design process. 
Our DeepRank not only significantly outperforms the deep learning baselines, but also significantly improves the results of learning to rank methods, even only use the query and document raw text data.

though SQA has used both automatically learned features and handcrafted features, the performance cannot compare with
DeepRank by using only automatically learned features for ranking.If we incorporate handcrafted features into DeepRank, the performance will be further improved, as shown in DeepRank-CNN-Feat.the results demonstrate the superiority of our deep architecture.

## 6. Detailed Analysis of DeepRank

DeepRank is such a flexible deep architecture that different parameter seings and neural networks can be used in the detection,
measurement, and aggregation steps. Some of these settings may largely influence the final ranking performances. therefore, we conduct a detailed analysis on MQ2007 to show the comparisons of DeepRank with different settings, with expect to give some insights in
for implementation. Specifically, we analyze four factors, i.e. window size of query-centric context in the detection step, input tensor the measurement step, neural network in the measurement step, positional function in the aggregation step. We change one factor of
the above DeepRank-CNN each time to conduct the comparisons.

### 6.1. mpact of Different Window Sizes of ery-Centric Context.

window size of query-centric context determines the scope of local relevance in the human judgment process. With a small window size, users would determine local relevance with less effort since contexts are short, but it is easy to introduce ambiguity due to
limited context information. When window size is large, there are suficient contexts to facilitate the precise local relevance judgment,
but the cost is also increased and many noises may inuence the judgment. We conduct an experiment to compare different window
sizes of query-centric context, varying in the range of 1, 7, 11, 15, 19 and 23. the results listed at the top of Table 2 show that the performances of DeepRank first increase and then become stable, with the increase of window size. e best performance is obtained with
window size up to 11/15.

### 6.2. Impact of Different Input Tensors

In order to capture both word representations of query/query-centric context and their interactions, we propose to construct a three-order tensor Sas the input of the measure network. Here, we compare four different settings of tensor. SindI and ScosI stand for the case when we use indicator or cosine function to construct the interaction matrix, and omit the other two matrices in the tensor. SR stands for the case that only word representations of query and query-centric context is considered in the tensor, i.e. interaction matrix is ignored. ScosI R stands for the case when we use the three-order tensor, which is exactly the DeepRank we used in the performance comparisons.From the results listed in the second row of Table 2, we can see the performances are improved when more information is modeled in the tensor. therefore, both word representations of query/query- centric context and word-level interactions are important to the relevance judgement.

### 6.3.Impact of Different Measure Networks.

In the model section, we demonstrate how to use CNN and 2D-GRU to conduct such measurement,mainly because these two kinds of neural networks have the ability to capture the proximity heuristics. Of course, you can also use other deep learning architectures, such as DNN. In this section, we conduct experiments on MQ2007 to compare the three different versions of DeepRank, denoted as DeepRank-DNN, DeepRank-CNN,and DeepRank-2DGRU. the experimental results in the third row of Table 2 show that DeepRank-CNN and DeepRank-2DGRU perform
much better than DeepRank-DNN. the reason lies in that CNN and 2D-GRU both have the ability to model the proximity heuristics, while DNN cannot because it is position sensitive, which is contradict with the position independent proximity heuristic.

### 6.4. Impact of Different Position Functions

Here we compare DeepRank with four different position functions, i.e. Constant, Linear,Reciprocal, and Exponential functions,DeepRank-Recip is the best, while DeepRank-Const is the worst.As for the other two functions, DeepRank-Exp perform comparable
with DeepRank-Recip, and DeepRank-Linear is a lile worse than DeepRank-Recip and DeepRank-Exp. the results indicate that top
positions are more important, which is consistent with many previous studies for relevance ranking in IR. As for the reason why
reciprocal and exponential function performs better than linear function, we think this is because MQ2007 is extracted from GOV
data, where title and abstraction information may play a dominant role in determining the relevance. therefore, the functions with
a long tail, as that in reciprocal or exponential function, will be favored. To sum up, the position function plays an important role
in DeepRank, and users should pay more aention to the choice, which need to be conducted by considering the characteristics of
different applications.

### 6.5. Relations to Previous Models.

We also would like to point out that DeepRank has a close relationship with previous models, such as BM25, MatchPyramid, and Match-SRNN. With some simplification, DeepRank can reduce to (or approximate) these models.

## 7. CONCLUSIONS AND FUTURE WORK

the results show that DeepRank signicantly outperform learning to rank methods and existing deep IR models, when most existing deep IR models perform much worse than learning to rank methods. To the best of our knowledge, DeepRank is the first deep IR model to outperform existing learning to rank models. We also give a detailed analysis on DeepRank to show insights on parameter settings for implementation.
For future work, we plan to investigate the differences between the automatically learned representations of DeepRank and effective features used in learning to rank, which may introduce some insights for architecture design of more powerful deep IR models.

## 8. DATA PREPROCESSING

For pre-processing, all the words in documents and queries are white-space tokenized, lower-cased, and stemmed using the Krovetz
stemmer. Stopword removal is performed on query and document words using the INQUERY stop list. Words occurred less than 5
times in the collection are removed from all the document.

## 9. CODE
We have released two versions of DeepRank. the original one is TextNet (https://github.com/pl8787/textnet-release), the newer one
is implemented in PyTorch (https://github.com/pl8787/DeepRank_PyTorch).

## 10 . adress of article

https://arxiv.org/pdf/1710.05649.pdf