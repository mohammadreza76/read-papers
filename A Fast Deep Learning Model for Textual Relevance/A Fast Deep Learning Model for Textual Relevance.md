# S4L: A Fast Deep Learning Model for Textual Relevance in Biomedical Information Retrieva
Publications in the life sciences are characterized by a large technical vocabulary, with many lexical and semantic variations for
expressing the same concept. Towards addressing the problem of relevance in biomedical literature search, we introduce a deep learning model for the relevance of a document’s text to a keyword style query. Limited by a relatively small amount of training data, the
model uses pre-trained word embeddings. With these, the model first computes a variable-length Delta matrix between the query and document, representing a difference between the two texts,which is then passed through a deep convolution stage followed by a deep feed-forward network to compute a relevance score. This results in a fast model suitable for use in an online search engine.The model is robust and outperforms comparable state-of-the-art deep learning approaches.

## 1.INTRODUCTION
PubMed®1 is a free online search engine covering over 27 million articles from biomedical and life sciences journals and other texts,
with about 1 million added each year.To train our model, we collected data from PubMed click logs, restricting this to relevance search instead of the default sort order by date. Removing author searches and disjunctive boolean expressions resulted in a training set of about 20k queries. Given the small size of this data, we pre-trained word embeddings using word2vec on the entire PubMed corpus,producing a vocabulary of about 200k. This large gap beween training data and vocabulary sizes highlights a major challenge: how to make the model robust? Our Delta deep learning model begins by computing a variable-sized ‘Delta’ matrix between a document and a query, comprising the
vector difference between document word embeddings and the closest matching query word, and three scalar similarity measures between the query and document. The document is truncated to control run-time cost. The Delta matrix is processed through a stacked convolutional network and pooled to a fixed length. This,together with a summary query match statistic, is processed by a feed-forward network to produce a relevance score. Pairwise loss is optimized for training. This approach produces a model that is both robust, and fast enough for use in a search engine.In addition to model robustness, we also wanted to address two common search engine problems: (i) the under-specified query problem, where even irrelevant documents have prominent presence of the query terms, and relevance requires analysis of the topics and semantics not directly specified in the query, and (ii) the term mismatch problem, which requires detection of related alternative terms or phrases in the document when the actual query terms are not in the document. Our experiments show the Delta
model outperforms traditional lexical match factors and some related state-of-the-art neural approaches.The next sections discuss some related work, followed by a description of the model, the experiments and evaluation of results,ending with some concluding remarks.

## 2.RELATED WORK
Traditional lexical Information Retrieval (IR) factors, like Okapi BM25 [33] and Query Likelihood [27], measure the prominence of query terms occurring in documents treated as bags of words.Neural approaches to text relevance attempt to go beyond exact matches of query terms in documents, and model a degree of semantic match as a complex function in a continuous space (goodreviews can be found in [28, 40]). We will discuss some related approaches here.Most neural models begin by mapping words to points embedded in a real space. A popular approach (e.g. [11, 13]), also used in our model, is to pre-train word embeddings, e.g. using word2vec[25, 26]. The benefit of this approach is that a much larger unlabeled corpus can be used to train the embeddings, and our ‘Delta matrix’ takes advantage of the semantic relationships captured in the vector differences between words.The simplest embeddings based model is Word Mover’s Distance
(WMD) [19], a non-parameterized model for text similarity that does not require any training. We use this as one of our baselines.
Pre-trained embeddings are not necessarily targeted for optimal relevance scores. Nalisnick et al. [31] also use the ‘input’ vectors
normally discarded by word2vec to overcome some of this limitation. The ‘DSSM’ models of [14, 36] take a different approach by mapping each word to a bag of letter tri-grams and combining the corresponding one-hot vectors. Xiong et al. [38] show that training word embeddings as part of the relevance model has a major impact on the performance of a relvance model. However this requires a large amount of training data. Diaz et al. [4] show that ‘locally trained’ embeddings on pseudo-relevance feedback documents can provide better results, while admitting that this approach is not “computationally convenient”.Neural relevance models also differ in how they process document and query text. Some (e.g. [9, 13, 14, 36]) process each document and query using separate ‘Siamese’ networks into independent
semantic vectors. A second stage then scores the similarity between these vectors. This approach is very attractive for search engines,
because the document vectors can be pre-processed and stored,and the query vector need be produced once before scoring the documents, significantly reducing the cost at query time. We use the recent model described in [35] as a baseline. Another approach to text matching first develops ‘local interactions’ by comparing all possible combinations of words and word sequences between the document and query texts, often starting with a document-query word similarity matrix. Examples are described in [11, 13, 23, 32, 38]. The authors in [11] argue that the local interaction based approach is better at capturing detail, especially exact query term matches, and in their experiments their ‘DRMM’ model outperforms many previous approaches. This is a more computationally intensive architecture that does not allow any pre-computation. We take a similar approach by pairing each document word with a single query word, followed by deep convo-
lutions to capture some related compositional semantics. Run-time cost in our approach is controlled by truncating the document. We
show that our approach outperforms the DRMM model.The ‘PACRR-firstk’ model in [15] also truncates the document,then processes the resulting similarity matrix through several 2D convolutional layers aimed at capturing n-gram similarities, followed by a recurrent layer, resulting in a fairly complex model.The ‘DUET’ model described in [29] combines a local interaction model with an independent semantic vector model, with the goal of combining the benefits of ‘exact match’ and embedding based word similairities. Our simpler approach explicitly targets run-time efficiency, and a variant of the Delta model combines some lexical factors (similar to [35]) to further improve ranking performance.


## 3. THE DELTA MODEL
The components of the Delta Relevance Model (figure 1) are described below. The unshaded blocks represent inputs to the model:two vectors of word indices, one each for the Document D and the Query Q, and a vector of query-document Lexical Match factors LDQ =lexmatch(D,Q)for some chosen lexical match function.The small size of the training data (∼20,000 queries) compared to the vocabulary size (∼200,000) prevented us from training word embeddings as part of the model training. We had to adapt the word vectors pre-trained using word2vec’s unsupervised approach, to the task of relevance prediction. The Delta model uses two techniques that help in this and thus learn a richer and more robust decision surface. Changing the input space from word embeddings to differences in word embeddings shifts the domain of the decision surface to coordinates relative to the query. In addition, the Delta model’s use of a stack of convolution layers instead of a single layer adds more non-linearities to help capture a complex decision surface, a technique successful in image recognition [37]. The convolution layers also extract relevance-match signals from text n-grams, and are much faster than a recurrent layer which has a similar goal.
![ Figure 1](https://github.com/mohammadreza76/read-papers/blob/main/A%20Fast%20Deep%20Learning%20Model%20for%20Textual%20Relevance/1.png)

### 3.1 Word Embeddings
We leveraged the large PubMed corpus of over 27 million documents to pre-train the word vectors, using the SkipGram Hierarchical
Softmax method of word2vec [26], with a window size of ±5, a minimum term-frequency of 101, and a word-vector size of V =300 (see [3] for experiments with different parameter settings for biomedical text). This resulted in a vocabulary of 207,716 words.Rare words were replaced with the generic “UNK” token, which was initialized to ∼U [−0.25,0.25], as in [35].Given a document word sequence D =⟨wd1 ,...,wdN ⟩and query text Q =⟨wq1 ,...,wqM ⟩, where wi are indices into the vocabulary,the Embeddings layer replaces each word with its vector, giving us De =⟨d1,...,dN ⟩,Qe =⟨q1,...,qM ⟩where each di ,qi ∈ RV ,and V is the size of the word embedding. If a document has fewer than N words, or the query fewer than M words, they are padded on the right with zeros. Longer documents are truncated, and M is the longest query length in our data (see section 4.1).

### 3.2 The Delta Stage
This is an unparameterized stage, responsible for computing the Delta Matrix between the Document and the Query as follows:
(1) Compute the Euclidean distance between each pair di ,qj .
(2) For each document word di , determine the closest query word
q∗i , using these distances.
(3) Compute the vector differences (di −q∗i ).
(4) Compute the Delta features: cosine(di ,q∗i ), |di −q∗i |and nor-
malized proximity similarity metric 1 −|di −q∗i |/(|di |+|q∗i |).
The output of this stage is ∆=⟨δ1,...,δN ⟩, a N ×(V +3)sized real matrix. All operations above are masked to ignore padding.

### 3.3 The Network
The trainable portion of the Delta model consists of a Convolutional Stage followed by a Feed-Forward Stage. The Convolutional stage
attempts to pick up significant contextual and n-gram similarity features. These are combined with the Lexical Match features LDQ ,
then processed by the Feed-Forward Stage to produce a final relevance score. The weights for the layers in these stages comprise
the trainable parameters for the model.A convolution operation [10, 21] has the parameters: border mode, number of filters or feature maps nf , filter width k and stride s. We use 1-dimensional convolution along the text width with border mode ‘same’ which implicitly pads the input on either side before convolving, and a stride s =1, resulting in an output of the same width as the input.The output of the final convolution layer is ‘max-pooled’ by taking the maximum of each of the nf output features along the text width dimension, yielding a vector of size nf .This output is combined with the Lexical Match features LDQ and sent to the Feed-Forward stage which is a series of NF layers.

### 3.4 Training The Model
The training data derived from PubMed’s click logs provides relevance levels for query-document pairs based on the number of
clicks they received (see next section for more details). The Delta relevance model is trained to give more relevant documents a higher
score by tuning its parameters Θto minimize the pairwise maximum margin loss. Given a query Q and two matching documents D+,D− where D+ has higher relevance to the query than D−, the loss for this triple is expressed as:
L(Q,D+,D−; Θ)=max(0,1 −s(Q,D+; Θ)+s(Q,D−; Θ))
where s(Q,D; Θ)is the relevance score produced by the Delta model for the query-document pair Q,D. The Adagrad [6] stochastic gradient descent method was used to train the model, using a mini-batch size of 256. In addition to early stopping, separate L2 regularization costs were added on the weights of the Convolutional and Feed-Forward stages, and a dropout layer was added before the max-pooling layer in the Convolutional stage. The regularization coefficients and dropout probability were tuned using the held out validation data. Adding dropout layers to the Feed-Forward stage was also tested but was not found to help.

## 4. EXPERIMENTAL SETUP
### 4.1 Data
We collected query-document pairs extracted from PubMed click logs over several months where users selected ‘Best Match’ (relevance) as the retrieval sort order and clicked on at least one document in the search results. We recorded the first page of results of up to 20 documents, supplemented with the clicked document if it was not on the first page. Since our primary goal was to improve relevance for simple keyword style queries, we discarded queries containing disjunctive expressions, faceted queries, and queries longer than 7 words. Log extracts were further restricted to queries with at least 21 documents, and at least 3 clicked documents.These filters reduced the logs to about 33,500 queries, which were randomly split to 60% training, and 20% each for validation and testing.

Relevance Levels. The relevance level assigned to each query-document pair extracted from click logs is a probability of relevance, scaled to the range [0, 100] so that the minimum possible non-zero relevance is 1. For each query-document pair Q,D,we accumulated over the collection period the number of click-throughs c(D,Q) from search results to the document summary page, whether the document’s full-text was available in PubMed If t (D) ∈ {0,1}, and the number of subsequent click-throughs cf t (D,Q)to the document’s full-text. These were used to derive a weighted click-count cw (Q,D)that rewarded documents for which full-text was requested without penalizing those for which full-text was not available.Finally, we scaled the non-zero relevance levels to the range (1,100] to get srel (Q,D). This ensured a minimum margin between documents of low relevance and no relevance, and also put a high penalty in the NDCG metric for ranking high relevance documents below low relevance ones.

Tokenization. Each document in our data had a Title and an Abstract. For the neural models, we concatenated these to form the document’s ‘Text’. All document and query text was tokenized by splitting on space and punctuation, while preserving abbreviations and numeric forms, followed by a conversion to lower-case. To further reduce the vocabulary size, all punctuation was discarded.While word2vec processed the tokenzed documents in sentences,the document input to the neural models was a flat sequence of words without sentence breaks or markers. The distribution of document text widths (nbr. of words) in the data is shown in figure 2a.

### 4.1 Configuration Settings for the Delta Model
With the maximum-margin loss function, there was no reason to constrain the range of the final layer’s activation function. We got best results using the Leaky Rectified Linear Unit (Leaky ReLU). The Leaky ReLU was also used as the activation function for all the other layers of the Feed-Forward and Convolutional stages.An earlier version of the Delta model is described in [30]. The main changes since then are: a simpler Delta Matrix, changes to the activation functions used in all the stages, training to a pairwise loss function with different sample weighting, and comprehensive test on a number of lexical features. These changes resulted in a ∼10% improvement in the NDCG metrics.

#### 4.1.1 Relevance-based Sample Weighting.
Best results were obtained by adding a weight to each (Q,D+,D−)training sample in the loss function by taking the square-root of the difference in the scaled-relevance levels of the two documents:
weight(Q,D+,D−)=(srel (Q,D+)−srel (Q,D−))^0.5

#### 4.1.2 Lexical Match Features
As an extension to the “word overlap measures” used in the SevMos model [35], we tested 18 features for use as the ‘Lexical Match Factors’ input to the Delta model:
(1) Proportion of unique Query words present in document Text.
(2) Proportion of unique Query bigrams present in doc Text.
(3) Jaccard Similarity between Query and document Text.
(4) IDF weighted version of (1).
(5) An IDF weighted version of Jaccard Similarity (3).
(6) BM25 on Query, document Title.
(7) BM25 on Query, document Abstract.
(8) BM25 on Query, document Text.
(9) Proportion of unique Query words present in document Title.
(10) Proportion of unique Query bigrams present in doc Title.
(11) Jaccard Similarity between Query and document Title.
(12) IDF weighted version of (9).
(13) An IDF weighted version of Jaccard Similarity (11).
(14) Proportion of unique Query words present in doc Abstract.
(15) Proportion of unique Query bigrams present in doc Abstract.
(16) Jaccard Similarity between Query and document Abstract.
(17) IDF weighted version of (14).
(18) An IDF weighted version of Jaccard Similarity (16).

To compute these factors, Queries and Documents were tokenized as described above, without the rare word conflation needed for computing word embeddings. Document Text refers to the combined Title and Abstract, each of these (as well as the Query) treated as a sequence of words with no truncation. These factors were selected based on the speed of their computation in a search engine.Factors (3, 5, 11, 13, 16, 18) were also used in [35].

### 4.2 Baselines
We compared the performance of the Delta deep learning model against some traditional bag-of-words based textual relevance factors, a distance measurement based on distributional representations of words, and a couple of recent neural network models.

#### 4.2.1 Lexical Factors
The second lexical factor we tested was Unigram Query Likelihood (UQLM), which estimates the probability with which the most likely random process that generated the bag-of-words representation of the document, would generate the query. It is based on a generative unigram language model that is a mixture of two multinomial models [27] based on the document and the corpus,combined using Dirichlet smoothing [20, 39]. 

#### 4.2.2 Word Mover’s Distance. 
Since all the neural models in our experiments started with pre-trained word embeddings, the Word Mover’s Distance (WMD) model [19] for text dis-similarity(score decreases with increasing similarity) was an obvious baseline approach. Based on the Earth Mover’s Distance [34] applied to a bag-of-words representation for text, it is a non-parameterized approach to determine the minimum amount of total transportation cost (sum of product of inter-word cost and amount transported) needed to convert one document into the other. It uses the Euclidean distance between the word-vector representations of two words as the cost of moving from one word to another. We only report metrics for WMD applied to the document Title without removal of stop-words,as it performed better than the other alternatives tested.

#### 4.2.3 The Severyn-Moschitti Model.
As a recent example of the Independent Semantic Vector approach, we implemented the relevance classification model described in [35], along with a few variations. The query and document are fed into separate Convolutional stages, each comprising a single convolution layer with 256 feature maps and a filter width of 5, followed by Dropout and Global Max-Pooling. A similarity measure is computed from these pooled outputs using a similarity weight matrix. The similarity measure,the pooled outpus, and some lexical match features (“overlap measures” in [35]) are fed into a Classifier stage consisting of a series of feed-forward layers. In our experiments, we provided the SevMos models with all 18 lexical match features described in section 4.2.2. Optimal values for the L2-regularization and Dropout probability hyper-parameters were determined by tuning on validation data, as described for the Delta model.We tested several variants of this model covering: replacing the single convolution layer with a 3-layer stack of convolutions of filter width 3, similar to the Delta model’s Convolutional stage;training the model as a classifier v/s a relevance scorer to the pairwise max-margin loss; and various sample-weighting schemes. Best results were obtained with the classification model using a 3-layer convolution stack and square-root weighting of samples. We report the metrics for this approach as the “SevMos-C3” model below, and the corresponding single convolution layer based classifier as the “SevMos-C1” model.

#### 4.2.4 The DRMM Model. 
The Deep Relevance Matching Model(DRMM) is a recent example of the Local Interaction approach to text relevance, described in [11] to outperform several previous neural models on the Robust04 and ClueWeb-09-Cat-B datasets. While it is a simple model with only 162 trainable parameters, it begins by computing the cosine similarity between the embeddings of each document and query word pair, which dominates the model’s computational cost. We implemented the DRMM model as described in [11], using the Krovetz word stemmer during text tokenization,stopwords removed from queries, and the CBOW method of [26]to compute word embeddings.We tested DRMM on increasing values of N (maximum document width) and found the ranking metrics stopped improving after a width of 200 words (figure 2b). DRMM uses the same pairwise loss function.
![ Figure 2b](https://github.com/mohammadreza76/read-papers/blob/main/A%20Fast%20Deep%20Learning%20Model%20for%20Textual%20Relevance/2.png)

## 5. EVALUATION
The good performance of SevMos-C3 over SevMos-C1 demonstrates the benefits of using a convolutional stack. Combining these elements with the Delta matrix implementation of the Local Interaction architecture yields even better results, as depicted in the metrics for Delta-32-Lex3.

### 5.1 Impact of Different Features
Next we look at how different aspects of the Delta model affected its performance. The Convolutional stage extracts match-related
features from word n-grams in the document. The parameter nf controls the number of such features, and its effect on ranking is charted in figure 3. Both NDCG.20 and MAP improved as nf increased till around 32 filters, and then performance leveled off and then dropped slightly. At larger number of filters, the model becomes more complex, but this increase in complexity does not continue to yield better performance. More complex models are more likely to overfit the training data, and perhaps learning rate decay techniques might help converge to a better solution, an area to be explored further. However since our goal was to construct a fast model for use in a search engine, we had a preference for smaller models, and nf =32 provided a good balance between speed and performance.

Table 6 compares four different versions of the Delta model with nf =32. The Delta Stage of the model computes, for each document word, a difference vector against the closest query word,and three ‘Delta features’: the cosine similarity, euclidean distance,and normalized proximity. The first two rows of table 6 show the impact of removing the difference vectors and the Delta features from the Delta-32 model, on the ranking metrics for the test data.Both show a significant drop in performance compared to Delta-32. Finally, as reviewed above, adding the 3 lexical match features resulted in a significant improvement for the Delta-32-Lex3 model over the Delta-32 model.

### 5.2 Evaluation as a Reranker
Run-time Cost. As discussed above, the Independent Semantic Vector approach like that used in the SevMos models [35] is particularly attractive for use in search engines, because the document semantic vectors (e.g. pooled output from the convolution stage in SevMos) can be pre-computed and stored, the query vector needs to be computed just once per search, and the remaining computation (the similarity measure and classifier stage in SevMos) is fairly small and fast. This caching is not possible in the Local Interaction approaches like the DRMM and Delta models. In these two models, the computation is dominated by the cost to compare each pair of document and query words, so we look to reducing the size of the document by truncation to control the computation.In scientific literature, authors are strongly motivated to provide a highly informative and noise-free document Title and Abstract,making it particularly amenable to this approach.

## 6. CONCLUSION
While deep learning models for text understanding have made dramatic gains in recent years, they have tended to be large and slow. The challenge in information retrieval is still one of combining predictive power with low run-time overhead. This is also true when the corpus is scientific literature.We described the Delta Relevance model, a new deep learning model for text relevance, targeted for information retrieval in biomedical science literature. The main innovation in the model is to base the modeling function on differences and distances between word embeddings, as captured in the Delta features, rather than directly using the embeddings themselves like most NLP approaches.Other researchers have shown the benefit of training word embeddings as part of the model. That was not feasible here since the training data had only 20k queries compared to a vocabulary size of 200k. Using Delta features as the input to the model helped adapt the pre-trained word embeddings to our task.To achieve our goal of fast run-time, we used a convolutional network rather than the recurrent approaches popular in most neural NLP models. Using a stack of narrow convolutional layers instead of a single wide convolution gave the model more power.We showed that the Delta model outperformed comparable recent approaches in ranking metrics when trained and evaluated on data derived from the PubMed search engine click logs. Wedemonstrated that the model was robust, despite being trained on
a relatively small amount of data, and the model was fast enough for use in an on-line search engine.The Delta model might be especially suited to scientific literature in taking advantage of the high quality of the document Title and first few sentences of the Abstract. We believe that the previously observed good performance of the DRMM approach was on documents that were quite noisy (they contain a lot of meta data in the document text). An area worth exploring further, for its potential in improving both prediction performance and run-time costs, is pre-processing a document to extract significant portions for evaluating relevance, thus reducing the size of the input at run-time.Another area to explore is the benefit from retaining sentence and grammatical structure in document text.